{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab464aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    # parse the dataset\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    ################ distilled from notebook 0 ################\n",
    "    # check the integrity\n",
    "    assert df.isna().any().any() == False, 'There is at least one missing value.'\n",
    "    assert df['Timestamp'].is_monotonic_increasing, 'Timestamp is not sorted.'\n",
    "\n",
    "    # type-cast\n",
    "    df['abstime'] = pd.to_datetime(df['Timestamp'], unit='s').round('us')\n",
    "    df['monotime'] = df['Timestamp'] - df['Timestamp'].min()\n",
    "    df['aid_int'] = df['Arbitration_ID'].map(lambda x: int(x, 16))\n",
    "    df['y'] = df['Class'].map({'Normal': 0, 'Attack': 1})\n",
    "\n",
    "    ################ distilled from notebook 1 ################\n",
    "    # calculate the stream-wise timedelta\n",
    "    df['Timedelta'] = df.groupby('Arbitration_ID')['Timestamp'].diff()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_submit1 = load_dataset('0_Preliminary/1_Submission/Pre_submit_D.csv')\n",
    "df_submit2 = load_dataset('0_Preliminary/1_Submission/Pre_submit_S.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f986bc",
   "metadata": {},
   "source": [
    "Implement the `detect()` function. Then, submit the detection result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d361ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** For submission set 1 ***\n",
      "[[39977     0]\n",
      " [34810     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5345    1.0000    0.6967     39977\n",
      "           1     0.0000    0.0000    0.0000     34810\n",
      "\n",
      "    accuracy                         0.5345     74787\n",
      "   macro avg     0.2673    0.5000    0.3483     74787\n",
      "weighted avg     0.2857    0.5345    0.3724     74787\n",
      "\n",
      "*** For submission set 2 ***\n",
      "[[33953     0]\n",
      " [30912     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5234    1.0000    0.6872     33953\n",
      "           1     0.0000    0.0000    0.0000     30912\n",
      "\n",
      "    accuracy                         0.5234     64865\n",
      "   macro avg     0.2617    0.5000    0.3436     64865\n",
      "weighted avg     0.2740    0.5234    0.3597     64865\n",
      "\n",
      "*** Overall result ***\n",
      "[[73930     0]\n",
      " [65722     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5294    1.0000    0.6923     73930\n",
      "           1     0.0000    0.0000    0.0000     65722\n",
      "\n",
      "    accuracy                         0.5294    139652\n",
      "   macro avg     0.2647    0.5000    0.3461    139652\n",
      "weighted avg     0.2803    0.5294    0.3665    139652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def detect(df):\n",
    "    df = df.copy()\n",
    "    df['y_predicted'] = 0\n",
    "\n",
    "    ########## implement your detection routine here ##########\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "    abstime_ceil = df['abstime'].dt.ceil('10ms')\n",
    "    y = df.groupby(abstime_ceil)['y'].max()\n",
    "    y_predicted = df.groupby(abstime_ceil)['y_predicted'].max()\n",
    "    return y, y_predicted\n",
    "\n",
    "\n",
    "def submit(*results):\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'*** For submission set {i + 1} ***')\n",
    "        print(confusion_matrix(*result))\n",
    "        print(classification_report(*result, zero_division=0, digits=4))\n",
    "\n",
    "    print('*** Overall result ***')\n",
    "    list_y = pd.concat([x[0] for x in results])\n",
    "    list_y_predicted = pd.concat([x[1] for x in results])\n",
    "    assert list_y.shape[0] == list_y_predicted.shape[0], 'Error. Num of record mismatch.'\n",
    "    print(confusion_matrix(list_y, list_y_predicted))\n",
    "    print(classification_report(list_y, list_y_predicted, zero_division=0, digits=4))\n",
    "\n",
    "\n",
    "result1 = detect(df_submit1)\n",
    "result2 = detect(df_submit2)\n",
    "\n",
    "submit(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
